{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxVmFvNrDNeV",
        "outputId": "d59c0857-dd69-4391-8f18-b9ec1d2b6339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XBmmSxOF3aQx",
        "outputId": "82bb9833-1013-4fbe-81c2-701c8918fa67"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "# os.chdir(\"/content/drive/MyDrive\")\n",
        "# %cd MELDData\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8z1aHsDfl91",
        "outputId": "608fdb18-fb71-45b2-dc10-fd4ddc7c3622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-08-29 08:18:14--  http://web.eecs.umich.edu/~mihalcea/downloads/MELD.Raw.tar.gz\n",
            "Resolving web.eecs.umich.edu (web.eecs.umich.edu)... 141.212.113.214\n",
            "Connecting to web.eecs.umich.edu (web.eecs.umich.edu)|141.212.113.214|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10878146150 (10G) [application/x-gzip]\n",
            "Saving to: ‚ÄòMELD.Raw.tar.gz‚Äô\n",
            "\n",
            "MELD.Raw.tar.gz     100%[===================>]  10.13G  10.4MB/s    in 16m 20s \n",
            "\n",
            "2025-08-29 08:34:33 (10.6 MB/s) - ‚ÄòMELD.Raw.tar.gz‚Äô saved [10878146150/10878146150]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://web.eecs.umich.edu/~mihalcea/downloads/MELD.Raw.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-pLtp9ugPM-",
        "outputId": "60bea02a-d778-455a-ab56-a223c3a354ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MELD.Raw/\n",
            "MELD.Raw/train.tar.gz\n",
            "MELD.Raw/dev.tar.gz\n",
            "MELD.Raw/test_sent_emo.csv\n",
            "MELD.Raw/._train_splits\n",
            "MELD.Raw/dev_sent_emo.csv\n",
            "MELD.Raw/README.txt\n",
            "MELD.Raw/test.tar.gz\n"
          ]
        }
      ],
      "source": [
        "## -x: This option signifies \"extract.\"\n",
        "## -f: This option specifies that the next argument will be the name of the archive file.\n",
        "## -v: To view the list of files being extracted during the process\n",
        "## The tar command is intelligent enough to automatically detect and handle various\n",
        "##compression formats without needing specific decompression flags for most modern versions.\n",
        "## if you encounter issues or are using an older tar version,\n",
        "#you might need to explicitly include the decompression flag\n",
        "## -z: flag indicates gzip decompression\n",
        "!tar -xvzf MELD.Raw.tar.gz ## just copy paste do not take so hard on yourself it can be any name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJgvpCXNiM6x"
      },
      "outputs": [],
      "source": [
        "# # -c :  extract the contents into the MELD.Raw/ folder uppercase\n",
        "# !tar -xvzf MELD.Raw/train.tar.gz -C MELD.Raw/\n",
        "# !tar -xvzf MELD.Raw/dev.tar.gz -C MELD.Raw/\n",
        "!tar -xvzf MELD.Raw/test.tar.gz -C MELD.Raw/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubgtynFNkWQQ"
      },
      "outputs": [],
      "source": [
        "## explore the folder structure\n",
        "# List contents of MELD.Raw directory\n",
        "# !ls MELD.Raw\n",
        "\n",
        "# # List contents of train_splits folder\n",
        "# !ls MELD.Raw/train_splits\n",
        "\n",
        "# # List contents of dev_splits folder\n",
        "# !ls MELD.Raw/dev_splits_complete\n",
        "\n",
        "# # List contents of test_splits folder\n",
        "# !ls MELD.Raw/output_repeated_splits_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "orXfxt-JmdK7",
        "outputId": "b17138b7-93e4-4722-bff7-498c9459b1de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (9989, 11)\n",
            "Dev shape: (1109, 11)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"# Check column names and data types\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Sr No.\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Utterance\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"You must\\u0092ve had your hands full.\",\n          \"My duties?  All right.\",\n          \"That I did. That I did.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Speaker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"The Interviewer\",\n          \"Chandler\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Emotion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"surprise\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"positive\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dialogue_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Utterance_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Season\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 8,\n        \"max\": 8,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Episode\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 21,\n        \"max\": 21,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          21\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"StartTime\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"00:16:21,940\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EndTime\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"00:16:23,442\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3f5b8c44-2f7c-4e31-9d6f-5c233e378784\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sr No.</th>\n",
              "      <th>Utterance</th>\n",
              "      <th>Speaker</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Dialogue_ID</th>\n",
              "      <th>Utterance_ID</th>\n",
              "      <th>Season</th>\n",
              "      <th>Episode</th>\n",
              "      <th>StartTime</th>\n",
              "      <th>EndTime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>also I was the point person on my company¬ís tr...</td>\n",
              "      <td>Chandler</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>00:16:16,059</td>\n",
              "      <td>00:16:21,731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>You must¬íve had your hands full.</td>\n",
              "      <td>The Interviewer</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>00:16:21,940</td>\n",
              "      <td>00:16:23,442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>That I did. That I did.</td>\n",
              "      <td>Chandler</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>00:16:23,442</td>\n",
              "      <td>00:16:26,389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>So let¬ís talk a little bit about your duties.</td>\n",
              "      <td>The Interviewer</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>00:16:26,820</td>\n",
              "      <td>00:16:29,572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>My duties?  All right.</td>\n",
              "      <td>Chandler</td>\n",
              "      <td>surprise</td>\n",
              "      <td>positive</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>00:16:34,452</td>\n",
              "      <td>00:16:40,917</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f5b8c44-2f7c-4e31-9d6f-5c233e378784')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3f5b8c44-2f7c-4e31-9d6f-5c233e378784 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3f5b8c44-2f7c-4e31-9d6f-5c233e378784');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-65afdd0d-fec6-460e-99ee-01adb79b190e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-65afdd0d-fec6-460e-99ee-01adb79b190e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-65afdd0d-fec6-460e-99ee-01adb79b190e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Sr No.                                          Utterance          Speaker  \\\n",
              "0       1  also I was the point person on my company¬ís tr...         Chandler   \n",
              "1       2                   You must¬íve had your hands full.  The Interviewer   \n",
              "2       3                            That I did. That I did.         Chandler   \n",
              "3       4      So let¬ís talk a little bit about your duties.  The Interviewer   \n",
              "4       5                             My duties?  All right.         Chandler   \n",
              "\n",
              "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
              "0   neutral   neutral            0             0       8       21   \n",
              "1   neutral   neutral            0             1       8       21   \n",
              "2   neutral   neutral            0             2       8       21   \n",
              "3   neutral   neutral            0             3       8       21   \n",
              "4  surprise  positive            0             4       8       21   \n",
              "\n",
              "      StartTime       EndTime  \n",
              "0  00:16:16,059  00:16:21,731  \n",
              "1  00:16:21,940  00:16:23,442  \n",
              "2  00:16:23,442  00:16:26,389  \n",
              "3  00:16:26,820  00:16:29,572  \n",
              "4  00:16:34,452  00:16:40,917  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load emotion_labeled training data\n",
        "train_df = pd.read_csv(\"/content/MELD.Raw/train_sent_emo.csv\")\n",
        "dev_df = pd.read_csv(\"/content/MELD.Raw/dev_sent_emo.csv\")\n",
        "test_df = pd.read_csv(\"MELD.Raw/test_sent_emo.csv\")\n",
        "\n",
        "\n",
        "# Display shape and first few rows\n",
        "print(\"Train shape:\", train_df.shape) # (9989, 11)\n",
        "print(\"Dev shape:\", dev_df.shape) # (1109, 11)\n",
        "print(\"Test shape:\", test_df.shape) # (2610, 11)\n",
        "\n",
        "train_df.head()\n",
        "# Check column names and data types\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68qdppktgUaa"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"MELD.Raw/test_sent_emo.csv\")\n",
        "print(\"Test shape:\", test_df.shape) # (2610, 11)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vDLhZYQp_i1",
        "outputId": "be7d6a3a-6a91-4d09-ec59-30b9ee446b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_df_type: Sr No.           int64\n",
            "Utterance       object\n",
            "Speaker         object\n",
            "Emotion         object\n",
            "Sentiment       object\n",
            "Dialogue_ID      int64\n",
            "Utterance_ID     int64\n",
            "Season           int64\n",
            "Episode          int64\n",
            "StartTime       object\n",
            "EndTime         object\n",
            "dtype: object\n",
            "Emotion categories: ['neutral' 'surprise' 'fear' 'sadness' 'joy' 'disgust' 'anger']\n",
            "Sentiment categories: ['neutral' 'positive' 'negative']\n",
            "Emotion distribution:\n",
            " Emotion\n",
            "neutral     4710\n",
            "joy         1743\n",
            "surprise    1205\n",
            "anger       1109\n",
            "sadness      683\n",
            "disgust      271\n",
            "fear         268\n",
            "Name: count, dtype: int64\n",
            "Sentiment distribution:\n",
            " Sentiment\n",
            "neutral     4710\n",
            "negative    2945\n",
            "positive    2334\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"train_df_type:\", train_df.dtypes)\n",
        "\n",
        "# List unique emotion and sentiment categories\n",
        "print(\"Emotion categories:\", train_df['Emotion'].unique())\n",
        "print(\"Sentiment categories:\", train_df['Sentiment'].unique())\n",
        "\n",
        "# Count distribution of emotions\n",
        "emotion_counts = train_df['Emotion'].value_counts()\n",
        "print(\"Emotion distribution:\\n\", emotion_counts)\n",
        "\n",
        "# Count distribution of sentiments\n",
        "sentiment_counts = train_df['Sentiment'].value_counts()\n",
        "print(\"Sentiment distribution:\\n\", sentiment_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "WciPHqbGrih3",
        "outputId": "9913bb9d-e2e3-4f73-e266-07743893ab50"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  Text Preprocessing\\n\\nThe text data in MELD is mostly clean, but for NLP tasks, you‚Äôll likely want to:\\n\\n    Lowercase all text for consistency\\n\\n    Remove punctuation or special characters (if not needed for emotion cues)\\n\\n    Tokenize sentences into words or subwords\\n\\n    Handle contractions (e.g., \"don\\'t\" ‚Üí \"do not\") if using rule-based models\\n\\n    Remove stopwords (optional, depending on your model)\\n\\n    Convert to embeddings using BERT, RoBERTa, or GloVe\\n\\nüîä Audio Preprocessing\\n\\nThe .wav files are raw audio clips. You‚Äôll need to:\\n\\n    Extract features like MFCCs, spectrograms, or use pretrained models like Wav2Vec\\n\\n    Normalize volume levels if needed\\n\\n    Trim silence or pad clips to a fixed length\\n\\n    Align audio with text using timestamps (already provided in MELD)\\n\\nüé≠ Label Preprocessing\\n\\n    Convert categorical labels (Emotion, Sentiment) to numerical format\\n\\n    Balance classes if you\\'re training a classifier (some emotions are underrepresented)\\n\\n    One-hot encode or use label encoding depending on your model\\n\\nüß© Multimodal Alignment\\n\\nIf you\\'re doing multimodal fusion (text + audio + visual), you‚Äôll need to:\\n\\n    Synchronize modalities using timestamps\\n\\n    Handle missing data (some utterances may lack audio or visual)\\n\\n    Standardize feature dimensions across modalities\\n    '"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"  Text Preprocessing\n",
        "\n",
        "The text data in MELD is mostly clean, but for NLP tasks, you‚Äôll likely want to:\n",
        "\n",
        "    Lowercase all text for consistency\n",
        "\n",
        "    Remove punctuation or special characters (if not needed for emotion cues)\n",
        "\n",
        "    Tokenize sentences into words or subwords\n",
        "\n",
        "    Handle contractions (e.g., \"don't\" ‚Üí \"do not\") if using rule-based models\n",
        "\n",
        "    Remove stopwords (optional, depending on your model)\n",
        "\n",
        "    Convert to embeddings using BERT, RoBERTa, or GloVe\n",
        "\n",
        "üîä Audio Preprocessing\n",
        "\n",
        "The .wav files are raw audio clips. You‚Äôll need to:\n",
        "\n",
        "    Extract features like MFCCs, spectrograms, or use pretrained models like Wav2Vec\n",
        "\n",
        "    Normalize volume levels if needed\n",
        "\n",
        "    Trim silence or pad clips to a fixed length\n",
        "\n",
        "    Align audio with text using timestamps (already provided in MELD)\n",
        "\n",
        "üé≠ Label Preprocessing\n",
        "\n",
        "    Convert categorical labels (Emotion, Sentiment) to numerical format\n",
        "\n",
        "    Balance classes if you're training a classifier (some emotions are underrepresented)\n",
        "\n",
        "    One-hot encode or use label encoding depending on your model\n",
        "\n",
        "üß© Multimodal Alignment\n",
        "\n",
        "If you're doing multimodal fusion (text + audio + visual), you‚Äôll need to:\n",
        "\n",
        "    Synchronize modalities using timestamps\n",
        "\n",
        "    Handle missing data (some utterances may lack audio or visual)\n",
        "\n",
        "    Standardize feature dimensions across modalities\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryxmehBUMEBz"
      },
      "outputs": [],
      "source": [
        "# import gdown\n",
        "\n",
        "# url = 'https://drive.google.com/file/d/1RjrYSMpXxg_6r_nUQaysaPyMsldLpMcb/view?usp=sharing'\n",
        "# output = 'meld.tar'\n",
        "# gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lauObg_jMwYI",
        "outputId": "752cba5a-1917-43d5-9957-effbfb583db4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tar: This does not look like a tar archive\n",
            "tar: Skipping to next header\n",
            "tar: Exiting with failure status due to previous errors\n"
          ]
        }
      ],
      "source": [
        "#!tar -xvf meld.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMVhKIoUNlGf",
        "outputId": "547cbc5a-5caf-4b30-bd91-19d85fa481c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "meld.tar: HTML document, ASCII text, with very long lines (39274)\n"
          ]
        }
      ],
      "source": [
        "# !file meld.tar ##Ah, that explains the extraction error!\n",
        "## The file you downloaded‚Äîmeld.tar‚Äîis actually an HTML document, not a real .tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovV8-LekN_CP",
        "outputId": "b2bcdd7d-5092-4c06-cf70-73fa4c7bb914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1RjrYSMpXxg_6r_nUQaysaPyMsldLpMcb\n",
            "From (redirected): https://drive.google.com/uc?id=1RjrYSMpXxg_6r_nUQaysaPyMsldLpMcb&confirm=t&uuid=282984b7-261d-4586-8611-dfda6d04ed14\n",
            "To: /content/meld.tar\n",
            "100% 2.29G/2.29G [00:32<00:00, 71.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1RjrYSMpXxg_6r_nUQaysaPyMsldLpMcb --fuzzy --no-cookies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D5YN3JoORAK",
        "outputId": "af6ebe73-350e-4baa-9c3f-3c06ec91deb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "meld.tar: POSIX tar archive (GNU)\n",
            "dev.pkl\n",
            "test.pkl\n",
            "train.pkl\n",
            "embedding.p\n"
          ]
        }
      ],
      "source": [
        "!file meld.tar ## it is correct now\n",
        "!tar -xvf meld.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTLlp72ROkWY",
        "outputId": "d851685a-06af-40fc-fa57-1be0b48be191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "with open('train.pkl', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "\n",
        "# Check type and structure\n",
        "print(type(train_data))  # likely list or dict\n",
        "# sample = train_data[0]   # or train_data['some_id'] if it's a dict\n",
        "\n",
        "# # Print keys and shapes\n",
        "# for key in sample:\n",
        "#     print(f\"{key}: type={type(sample[key])}, shape={getattr(sample[key], 'shape', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTNQ0DHyPkkW",
        "outputId": "953dcb00-1236-4862-db8e-4ce13aaf96f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: 9988\n",
            "First key: 0_0\n",
            "token_ids: type=<class 'list'>, shape=N/A\n",
            "audio_features: type=<class 'numpy.ndarray'>, shape=(19, 32)\n",
            "video_features: type=<class 'numpy.ndarray'>, shape=(19, 2048)\n",
            "label: type=<class 'int'>, shape=N/A\n"
          ]
        }
      ],
      "source": [
        "# Print the number of samples\n",
        "print(\"Number of samples:\", len(train_data))\n",
        "\n",
        "# Print the first key\n",
        "first_key = list(train_data.keys())[0]\n",
        "print(\"First key:\", first_key)\n",
        "\n",
        "# Print the structure of one sample\n",
        "sample = train_data[first_key]\n",
        "for k, v in sample.items():\n",
        "    print(f\"{k}: type={type(v)}, shape={getattr(v, 'shape', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNTaYLEHQ2mX"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Number of samples: 9988 You have 9,988 data\n",
        " points‚Äîlikely utterances or dialogue turns\n",
        " if you're working with something like MELD or another multimodal emotion dataset.\n",
        "\n",
        "First key: 0_0 This is probably the ID for the first sample.\n",
        "It might represent something like conversationID_utteranceID.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOvOYNshS2aX"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# plt.figure(figsize=(12, 4))\n",
        "# plt.plot(sample['video_features'])\n",
        "# plt.title('Video Feature Vector')\n",
        "# plt.xlabel('Time Frame')\n",
        "# plt.ylabel('Feature Value')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MdkS2eCS25O"
      },
      "outputs": [],
      "source": [
        "# ## this visualization can be messy so i recommend to :\n",
        "# plt.plot(sample['video_features'][:, 0])\n",
        "# plt.title('Video Feature - Dimension 0')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE5H2f-6TsAb"
      },
      "outputs": [],
      "source": [
        "# plt.imshow(sample['video_features'].T, aspect='auto', origin='lower', cmap='plasma')\n",
        "# plt.title('Video Feature Map')\n",
        "# plt.xlabel('Time Frame')\n",
        "# plt.ylabel('Feature Dimension')\n",
        "# plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "HWjUJo-XTxWZ",
        "outputId": "c8fb463f-fd4d-4ca2-a7ac-fb129ac8160b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntoken_ids are not features yet. They are just numerical representations of words or subwords from the original utterance in meld.raw.\\ntoken_ids\\tInteger IDs for each word/subword\\tInput to text encoder\\nText encoder\\tModel that converts IDs to feature vectors\\tProduces semantic text features\\nRaw text\\tOriginal utterance from MELD\\tUsed for alignment and debugging\\n'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "token_ids are not features yet. They are just numerical representations of words or subwords from the original utterance in meld.raw.\n",
        "token_ids\tInteger IDs for each word/subword\tInput to text encoder\n",
        "Text encoder\tModel that converts IDs to feature vectors\tProduces semantic text features\n",
        "Raw text\tOriginal utterance from MELD\tUsed for alignment and debugging\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZqpqHXhWOam"
      },
      "outputs": [],
      "source": [
        "# raw_lookup = {}\n",
        "# for row in meld.Raw:  # or use meld_raw.iterrows() if it's a DataFrame\n",
        "#     key = f\"{row['Dialogue_ID']}_{row['Utterance_ID']}\"\n",
        "#     raw_lookup[key] = row\n",
        "## we have Meld.Raw as dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLUABlRhXWi-"
      },
      "source": [
        "### build look up table for align with extracted multimodal features so you can train a model from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5hbJ2tkXQBg"
      },
      "outputs": [],
      "source": [
        "train_df['key'] = train_df['Dialogue_ID'].astype(str) + '_' + train_df['Utterance_ID'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B80kzIJjXTuY"
      },
      "outputs": [],
      "source": [
        "raw_lookup = {\n",
        "    row['key']: row\n",
        "    for _, row in train_df.iterrows()\n",
        "}\n",
        "\n",
        "## compact and powerful way to build a lookup dictionary from your train_df DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEsQ_DiMXrqV",
        "outputId": "cadf6bb4-c782-47bb-eb5a-6adfd97d4562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sr No.                                                          1\n",
            "Utterance       also I was the point person on my company¬ís tr...\n",
            "Speaker                                                  Chandler\n",
            "Emotion                                                   neutral\n",
            "Sentiment                                                 neutral\n",
            "Dialogue_ID                                                     0\n",
            "Utterance_ID                                                    0\n",
            "Season                                                          8\n",
            "Episode                                                        21\n",
            "StartTime                                            00:16:16,059\n",
            "EndTime                                              00:16:21,731\n",
            "key                                                           0_0\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(raw_lookup['0_0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "CfJYtWx4YcC6",
        "outputId": "66ad729c-c711-449e-ce0e-abd035ce0c22"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nSo No Mixing Happens\\n\\n    Each row is stored under its own unique key.\\n\\n    No rows are ‚Äúcombined‚Äù or ‚Äúconcatenated‚Äù across seasons.\\n\\n    You can still access season/episode info from the row itself.'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Imagine you have a spreadsheet with thousands of rows. Each row has:\n",
        "\n",
        "    A conversation ID (Dialogue_ID)\n",
        "\n",
        "    A line number (Utterance_ID)\n",
        "\n",
        "    A season and episode\n",
        "\n",
        "    A speaker and their line\n",
        "\n",
        "You create a new column called key that says \"DialogueID_UtteranceID\"‚Äîlike \"12_4\".\n",
        "\n",
        "Now you build a dictionary where each key is that \"12_4\" and the value is the full row. That‚Äôs all this code is doing.\n",
        "\n",
        "So No Mixing Happens\n",
        "\n",
        "    Each row is stored under its own unique key.\n",
        "\n",
        "    No rows are ‚Äúcombined‚Äù or ‚Äúconcatenated‚Äù across seasons.\n",
        "\n",
        "    You can still access season/episode info from the row itself.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKd-3HUObzKy"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "‚úÖ Loaded and explored the MELD metadata\n",
        "‚úÖ Downloaded and unpacked the preprocessed multimodal features (train.pkl)\n",
        "‚úÖ Created a key to align metadata with feature samples\n",
        "‚úÖ Built a raw_lookup dictionary for fast access to metadata by key\n",
        "\n",
        "Now you're ready to combine the metadata (train_df) with the extracted features (train_data) into a unified dataset class for training your fusion model '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W-nW7zLFO36",
        "outputId": "4314d07d-9754-47f6-bde3-817bcda3f807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text shape: torch.Size([50])\n",
            "Audio shape: torch.Size([19, 32])\n",
            "Video shape: torch.Size([19, 2048])\n",
            "Label: tensor(0)\n",
            "Utterance: also I was the point person on my company¬ís transition from the KL-5 to GR-6 system.\n",
            "Emotion: neutral\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MELDFusionDataset(Dataset):\n",
        "    def __init__(self, features_dict, raw_lookup, max_text_len=50, pad_token_id=0):\n",
        "        self.keys = list(features_dict.keys())\n",
        "        self.features = features_dict\n",
        "        self.raw_lookup = raw_lookup\n",
        "        self.max_text_len = max_text_len\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        key = self.keys[idx]\n",
        "        feat = self.features[key]\n",
        "        raw = self.raw_lookup.get(key, None)\n",
        "\n",
        "        # Pad or truncate token_ids\n",
        "        token_ids = feat['token_ids']\n",
        "        if len(token_ids) < self.max_text_len:\n",
        "            token_ids += [self.pad_token_id] * (self.max_text_len - len(token_ids))\n",
        "        else:\n",
        "            token_ids = token_ids[:self.max_text_len]\n",
        "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "        # Audio and video features\n",
        "        audio = torch.tensor(feat['audio_features'], dtype=torch.float)\n",
        "        video = torch.tensor(feat['video_features'], dtype=torch.float)\n",
        "\n",
        "        # Label\n",
        "        label = torch.tensor(feat['label'], dtype=torch.long)\n",
        "\n",
        "        # Optional: include raw metadata\n",
        "        return {\n",
        "            'key': key,\n",
        "            'token_ids': token_ids,\n",
        "            'audio': audio,\n",
        "            'video': video,\n",
        "            'label': label,\n",
        "            'utterance': raw['Utterance'] if raw is not None else None,\n",
        "            'emotion': raw['Emotion'] if raw is not None else None,\n",
        "            'speaker': raw['Speaker'] if raw is not None else None\n",
        "        }\n",
        "dataset = MELDFusionDataset(train_data, raw_lookup)\n",
        "sample = dataset[0]\n",
        "\n",
        "print(\"Text shape:\", sample['token_ids'].shape)\n",
        "print(\"Audio shape:\", sample['audio'].shape)\n",
        "print(\"Video shape:\", sample['video'].shape)\n",
        "print(\"Label:\", sample['label'])\n",
        "print(\"Utterance:\", sample['utterance'])\n",
        "print(\"Emotion:\", sample['emotion'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "637CvZWv2HjH"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Downloaded and extracted the MELD dataset\n",
        "\n",
        "Loaded and explored the emotion-labeled CSV files\n",
        "\n",
        "Pulled in preprocessed features from a pickle file\n",
        "\n",
        "Created a lookup dictionary for raw metadata\n",
        "\n",
        "Defined a custom PyTorch Dataset class to fuse modalities '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M6QHdbSFWlQ"
      },
      "outputs": [],
      "source": [
        "## But here‚Äôs the issue: raw is a pandas Series (a row from your DataFrame), and Python doesn‚Äôt know how to evaluate if raw: because it‚Äôs not a single boolean‚Äîit‚Äôs a whole row of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcANTr3RJ6ff"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def meld_collate_fn(batch):\n",
        "    keys = [item['key'] for item in batch]\n",
        "    token_ids = torch.stack([item['token_ids'] for item in batch])\n",
        "    labels = torch.stack([item['label'] for item in batch])\n",
        "\n",
        "    # Pad audio and video sequences\n",
        "    audio_seqs = [item['audio'] for item in batch]\n",
        "    video_seqs = [item['video'] for item in batch]\n",
        "\n",
        "    padded_audio = pad_sequence(audio_seqs, batch_first=True)  # shape: [B, T, F]\n",
        "    padded_video = pad_sequence(video_seqs, batch_first=True)  # shape: [B, T, F]\n",
        "\n",
        "    # Optional metadata\n",
        "    utterances = [item['utterance'] for item in batch]\n",
        "    emotions = [item['emotion'] for item in batch]\n",
        "    speakers = [item['speaker'] for item in batch]\n",
        "\n",
        "    return {\n",
        "        'keys': keys,\n",
        "        'token_ids': token_ids,\n",
        "        'audio': padded_audio,\n",
        "        'video': padded_video,\n",
        "        'labels': labels,\n",
        "        'utterances': utterances,\n",
        "        'emotions': emotions,\n",
        "        'speakers': speakers\n",
        "    }\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=meld_collate_fn\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbt3UxduKBVQ",
        "outputId": "234f56f2-7ba7-4214-ac62-4408aa76a6a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch token_ids shape: torch.Size([4, 50])\n",
            "Batch audio shape: torch.Size([4, 15, 32])\n",
            "Batch video shape: torch.Size([4, 15, 2048])\n",
            "Batch labels: tensor([3, 4, 0, 4])\n",
            "\n",
            "Sample utterances:\n",
            "Emotion: sadness | Utterance: Umm, well I sorta have some bad news, can I come in?\n",
            "Emotion: joy | Utterance: I am going to take you out to dinner tonight.\n",
            "Emotion: neutral | Utterance: I got it. Uh, Joey, women don't have Adam's apples.\n",
            "Emotion: joy | Utterance: Great!\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(train_loader))\n",
        "\n",
        "print(\"Batch token_ids shape:\", batch['token_ids'].shape)  # [B, T]\n",
        "print(\"Batch audio shape:\", batch['audio'].shape)          # [B, T, F]\n",
        "print(\"Batch video shape:\", batch['video'].shape)          # [B, T, F]\n",
        "print(\"Batch labels:\", batch['labels'])                    # [B]\n",
        "\n",
        "print(\"\\nSample utterances:\")\n",
        "for utt, emo in zip(batch['utterances'], batch['emotions']):\n",
        "    print(f\"Emotion: {emo} | Utterance: {utt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "fiwh5HL_F6xf",
        "outputId": "0bbc6fe4-c8a9-4557-edd6-36579675c0f6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAwesome! Let‚Äôs build a clean pipeline to handle batches from your MELDFusionDataset. We‚Äôll create:\\n\\n    A custom collate function to pad variable-length audio/video.\\n\\n    A DataLoader to batch your data.\\n\\n    A quick visualization of a batch to inspect shapes and contents.'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Awesome! Let‚Äôs build a clean pipeline to handle batches from your MELDFusionDataset. We‚Äôll create:\n",
        "\n",
        "    A custom collate function to pad variable-length audio/video.\n",
        "\n",
        "    A DataLoader to batch your data.\n",
        "\n",
        "    A quick visualization of a batch to inspect shapes and contents.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgaZ9uNOH08f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FusionEmotionModel(nn.Module):\n",
        "    def __init__(self, text_dim, audio_dim, video_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # Temporal modeling for audio/video\n",
        "        self.audio_rnn = nn.GRU(audio_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.video_rnn = nn.GRU(video_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # Text projection\n",
        "        self.text_fc = nn.Sequential(\n",
        "            nn.Linear(text_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Attention layers\n",
        "        self.attn_audio = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.attn_video = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "        # # Final fusion\n",
        "        # self.classifier = nn.Sequential(\n",
        "        #     nn.Linear(hidden_dim * 4, hidden_dim),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Dropout(0.3),\n",
        "        #     nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(640, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes))\n",
        "\n",
        "    def attention_pooling(self, rnn_out, attn_layer):\n",
        "        weights = F.softmax(attn_layer(rnn_out), dim=1)  # [B, T, 1]\n",
        "        pooled = torch.sum(weights * rnn_out, dim=1)     # [B, hidden]\n",
        "        return pooled\n",
        "\n",
        "    def forward(self, text_embed, audio_seq, video_seq):\n",
        "        # Text\n",
        "        text_feat = self.text_fc(text_embed)  # [B, hidden]\n",
        "\n",
        "        # Audio\n",
        "        audio_out, _ = self.audio_rnn(audio_seq)  # [B, T, 2*hidden]\n",
        "        audio_feat = self.attention_pooling(audio_out, self.attn_audio)\n",
        "\n",
        "        # Video\n",
        "        video_out, _ = self.video_rnn(video_seq)  # [B, T, 2*hidden]\n",
        "        video_feat = self.attention_pooling(video_out, self.attn_video)\n",
        "\n",
        "        # Fusion\n",
        "        fused = torch.cat([text_feat, audio_feat, video_feat], dim=1)\n",
        "        return self.classifier(fused)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5e76orYJm-z"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def detailed_report(preds, labels, emotion_labels):\n",
        "    preds = preds.argmax(dim=1).cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "    print(classification_report(labels, preds, target_names=emotion_labels))\n",
        "\n",
        "\n",
        "def evaluate_metrics(preds, labels):\n",
        "    preds = preds.argmax(dim=1).cpu().numpy()\n",
        "    labels = labels.cpu().numpy()\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')  # or 'macro' for equal class weight\n",
        "    return acc, f1\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def get_text_embedding(token_ids):\n",
        "    with torch.no_grad():\n",
        "        output = bert(token_ids)\n",
        "        return output.last_hidden_state.mean(dim=1)  # [B, hidden_size]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dqbwcZ4LYOe"
      },
      "outputs": [],
      "source": [
        "model = FusionEmotionModel(\n",
        "    text_dim=768,     # BERT hidden size\n",
        "    audio_dim=32,     # your audio feature size\n",
        "    video_dim=2048,   # your video feature size\n",
        "    hidden_dim=128,   # can be tuned\n",
        "    num_classes=7     # adjust based on your emotion labels\n",
        ")\n",
        "model = FusionEmotionModel(\n",
        "    text_dim=768,     # BERT hidden size\n",
        "    audio_dim=32,     # your audio feature size\n",
        "    video_dim=2048,   # your video feature size\n",
        "    hidden_dim=128,   # can be tuned\n",
        "    num_classes=7     # adjust based on your emotion labels\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "token_ids = batch['token_ids'].to(device)\n",
        "audio = batch['audio'].to(device)\n",
        "video = batch['video'].to(device)\n",
        "labels = batch['labels'].to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "YozqHjElJtOl",
        "outputId": "2eaf6bc6-fdab-4a0c-c23d-983ca4118701"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nTrains your FusionEmotionModel using your train_loader\\n\\nComputes loss and metrics\\n\\nSaves a checkpoint every 5 epochs to Google Drive'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "Trains your FusionEmotionModel using your train_loader\n",
        "\n",
        "Computes loss and metrics\n",
        "\n",
        "Saves a checkpoint every 5 epochs to Google Drive'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r704eUOHLXa_",
        "outputId": "f75423c7-b1ec-4674-9291-42126f71e3d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "checkpoint_dir = \"/content/drive/MyDrive/emotion_model_checkpoints\"\n",
        "import os\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK3L3dyrOgA2"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lztPcHtOb00"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"/content/drive/MyDrive/emotion_model_checkpoints/model_epoch_10.pt\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56rnTBrIKP3H",
        "outputId": "48b12bd2-d4f9-43f5-9218-b800c7efbd07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [32:54<00:00,  1.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss=3787.2110, Acc=0.4716, F1=0.3030\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_1.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [34:46<00:00,  1.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss=3675.9479, Acc=0.4829, F1=0.3420\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_2.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [32:15<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Loss=3563.4865, Acc=0.5030, F1=0.3950\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_3.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [32:04<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Loss=3485.1746, Acc=0.5175, F1=0.4251\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_4.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [32:02<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss=3434.4700, Acc=0.5286, F1=0.4435\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_5.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [32:09<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Loss=3404.1840, Acc=0.5366, F1=0.4583\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_6.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [32:01<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Loss=3364.1258, Acc=0.5446, F1=0.4671\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_7.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [32:30<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Loss=3352.3863, Acc=0.5432, F1=0.4685\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_8.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [31:53<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Loss=3316.5415, Acc=0.5503, F1=0.4784\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_9.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [31:58<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Loss=3308.4662, Acc=0.5491, F1=0.4783\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_10.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [31:47<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Loss=3263.3937, Acc=0.5541, F1=0.4856\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_11.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [31:57<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Loss=3260.0736, Acc=0.5572, F1=0.4914\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_12.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [31:53<00:00,  1.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Loss=3243.4623, Acc=0.5545, F1=0.4877\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_13.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [33:40<00:00,  1.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Loss=3228.4455, Acc=0.5615, F1=0.4982\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_14.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [32:20<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Loss=3205.8545, Acc=0.5610, F1=0.4982\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_15.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [32:18<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Loss=3187.5707, Acc=0.5606, F1=0.4971\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_16.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [32:09<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Loss=3154.2910, Acc=0.5678, F1=0.5073\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_17.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2497/2497 [32:05<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Loss=3136.2223, Acc=0.5681, F1=0.5087\n",
            "‚úÖ Saved checkpoint to /content/drive/MyDrive/emotion_model_checkpoints/model_epoch_18.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1226/2497 [15:45<17:52,  1.19it/s]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    ##for batch in train_loader: ###batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "        token_ids = batch['token_ids'].to(device)\n",
        "        audio = batch['audio'].to(device)\n",
        "        video = batch['video'].to(device)\n",
        "        labels = batch['labels'].squeeze().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        text_embed = get_text_embedding(token_ids)\n",
        "        outputs = model(text_embed, audio, video)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        all_preds.append(outputs.detach())\n",
        "        all_labels.append(labels.detach())\n",
        "\n",
        "    # Metrics\n",
        "    preds = torch.cat(all_preds)\n",
        "    labels = torch.cat(all_labels)\n",
        "    acc, f1 = evaluate_metrics(preds, labels)\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss={total_loss:.4f}, Acc={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "    # Save checkpoint every 5 epochs\n",
        "    if epoch % 1 == 0:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': total_loss\n",
        "        }, checkpoint_path)\n",
        "        print(f\"‚úÖ Saved checkpoint to {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQC4yzf-6S8p"
      },
      "outputs": [],
      "source": [
        "### for get better loss\n",
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "        token_ids = batch['token_ids'].to(device)\n",
        "        audio = batch['audio'].to(device)\n",
        "        video = batch['video'].to(device)\n",
        "        labels = batch['labels'].squeeze().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        text_embed = get_text_embedding(token_ids)\n",
        "        outputs = model(text_embed, audio, video)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        all_preds.append(outputs.detach())\n",
        "        all_labels.append(labels.detach())\n",
        "\n",
        "    # Normalize loss\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # Metrics\n",
        "    preds = torch.cat(all_preds)\n",
        "    labels = torch.cat(all_labels)\n",
        "    acc, f1 = evaluate_metrics(preds, labels)\n",
        "\n",
        "    print(f\"üìä Epoch {epoch}: Avg Loss = {avg_loss:.4f}, Accuracy = {acc:.4f}, F1 Score = {f1:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch}.pt\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': avg_loss\n",
        "    }, checkpoint_path)\n",
        "    print(f\"‚úÖ Saved checkpoint to {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs6fGTrR2wX1"
      },
      "outputs": [],
      "source": [
        "## Epoch 13: Loss=3243.4623, Acc=0.5545, F1=0.4877\n",
        "'''\n",
        "You're building a fantastic multimodal emotion recognition pipeline using the\n",
        " MELD dataset‚Äîtext, audio, and video fused with attention and GRU layers.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9puf5w9hpB_"
      },
      "outputs": [],
      "source": [
        "total_loss = 0\n",
        "for batch in dataloader:\n",
        "    outputs = model(batch['input'])\n",
        "    loss = criterion(outputs, batch['label'])\n",
        "    total_loss += loss.item()\n",
        "\n",
        "avg_loss = total_loss / len(dataloader)  # Normalize by number of batches\n",
        "avg_loss = total_loss / total_samples\n",
        "##If You're Using a Custom Loss Function\n",
        "\n",
        "def custom_loss(pred, target):\n",
        "    loss = torch.sum((pred - target)**2)\n",
        "    return loss / pred.size(0)  # Normalize by batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFd1Vm2lOl6S"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Yes, exactly‚Äîthat‚Äôs the goal of your model! üéØ\n",
        "\n",
        "You're building a multimodal emotion recognition model that takes in:\n",
        "\n",
        "    Text (via BERT embeddings)\n",
        "\n",
        "    Audio features (like tone, pitch, etc.)\n",
        "\n",
        "    Video features (like facial expressions or gestures)\n",
        "\n",
        "‚Ä¶and fuses them together to predict one of several emotion classes (like happy, sad, angry, etc.).'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kYjp8jqPkHT"
      },
      "outputs": [],
      "source": [
        "''' üî• Why This Model Works for Emotion Prediction\n",
        "\n",
        "    Text captures what‚Äôs being said\n",
        "\n",
        "    Audio captures how it‚Äôs being said\n",
        "\n",
        "    Video captures non-verbal cues like facial expressions\n",
        "\n",
        "    Attention pooling helps the model focus on the most emotionally relevant parts of audio/video sequences\n",
        "\n",
        "    Fusion layer combines all modalities into a single representation for classification\n",
        "\n",
        "So yes‚Äîonce trained, your model will take a new sample (text + audio + video) and output a predicted emotion label.'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j37qSHesQC_Q"
      },
      "outputs": [],
      "source": [
        "### multimodal emotion recognition model.\n",
        "'''Parameter\tSuggested Value\tWhy It Works\n",
        "Learning Rate (lr)\t1e-4 or 5e-5\tSmall enough for BERT fine-tuning, stable for GRUs\n",
        "Epochs\t15‚Äì30\tEnough to converge without overfitting\n",
        "Batch Size\t4‚Äì16 (you‚Äôre using 4)\tSmall batches help generalization, especially with limited data\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0bg3MFcQYoX"
      },
      "outputs": [],
      "source": [
        "'''The good news? You‚Äôve now got a full training loop, checkpointing, and visualization tools all wired up. Once this run finishes, you‚Äôll have a model that not only learns but also shows you how it's learning‚Äîand saves itself along the way.\n",
        "\n",
        "If you want to speed up future experiments, I can help you:\n",
        "\n",
        "    Add validation and early stopping\n",
        "\n",
        "    Automate hyperparameter sweeps\n",
        "\n",
        "    Run faster with mixed precision or gradient accumulation'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Cz5lP8BRhiJ"
      },
      "outputs": [],
      "source": [
        "'''‚úÖ Additions to Your Training Loop\n",
        "1. Validation Step\n",
        "\n",
        "Evaluate model performance on a separate validation set each epoch.\n",
        "2. Early Stopping\n",
        "\n",
        "Stop training if validation loss doesn‚Äôt improve for a set number of epochs.\n",
        "3. Best Model Saving\n",
        "\n",
        "Save the model with the best validation F1 score, not just every 5 epochs.\n",
        "4. Progress Bar\n",
        "\n",
        "Use tqdm to show live progress during each epoch.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLGkBzmYRjkh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "best_f1 = 0\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_f1s = []\n",
        "val_f1s = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch} [Training]\"):\n",
        "        token_ids = batch['token_ids'].to(device)\n",
        "        audio = batch['audio'].to(device)\n",
        "        video = batch['video'].to(device)\n",
        "        labels = batch['labels'].squeeze().to(device)\n",
        "\n",
        "        text_embed = get_text_embedding(token_ids)\n",
        "        outputs = model(text_embed, audio, video)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        all_preds.append(outputs.detach())\n",
        "        all_labels.append(labels.detach())\n",
        "\n",
        "    train_preds = torch.cat(all_preds)\n",
        "    train_labels = torch.cat(all_labels)\n",
        "    train_acc, train_f1 = evaluate_metrics(train_preds, train_labels)\n",
        "\n",
        "    train_losses.append(total_loss)\n",
        "    train_f1s.append(train_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch} ‚Äî Train Loss: {total_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch} [Validation]\"):\n",
        "            token_ids = batch['token_ids'].to(device)\n",
        "            audio = batch['audio'].to(device)\n",
        "            video = batch['video'].to(device)\n",
        "            labels = batch['labels'].squeeze().to(device)\n",
        "\n",
        "            text_embed = get_text_embedding(token_ids)\n",
        "            outputs = model(text_embed, audio, video)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            val_preds.append(outputs)\n",
        "            val_labels.append(labels)\n",
        "\n",
        "    val_preds = torch.cat(val_preds)\n",
        "    val_labels = torch.cat(val_labels)\n",
        "    val_acc, val_f1 = evaluate_metrics(val_preds, val_labels)\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    val_f1s.append(val_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch} ‚Äî Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_f1 > best_f1:\n",
        "        best_f1 = val_f1\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "        torch.save(best_model, os.path.join(checkpoint_dir, \"best_model.pt\"))\n",
        "        print(\"‚úÖ Best model saved!\")\n",
        "\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"‚èπÔ∏è Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    # Save checkpoint every 5 epochs\n",
        "    if epoch % 5 == 0:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': total_loss\n",
        "        }, checkpoint_path)\n",
        "        print(f\"üì¶ Checkpoint saved at {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBlGOhhMTVZU"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "save best model :\n",
        "üîç What Most Practitioners Do\n",
        "\n",
        "    Classification with imbalance: Save based on F1 score.\n",
        "\n",
        "    Classification with balance: Save based on accuracy or loss.\n",
        "\n",
        "    Regression or generative tasks: Save based on validation loss.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXl3uft5cIRI"
      },
      "outputs": [],
      "source": [
        "''' your fusion model is designed to predict exactly those seven emotions: Anger, Disgust, Sadness, Joy, Neutral, Surprise, and Fear. That‚Äôs a classic set for emotion recognition tasks, especially in datasets like MELD, IEMOCAP, or EmoReact.\n",
        "\n",
        "Here‚Äôs how your model is set up to handle that:\n",
        "üéØ Why Your Model Is Well-Suited for This Task\n",
        "\n",
        "    Multimodal Inputs: You're combining text, audio, and video‚Äîwhich is crucial because emotions are expressed not just in words, but in tone and facial expressions.\n",
        "\n",
        "    Temporal Modeling: Using GRUs for audio and video lets your model capture how emotions evolve over time within an utterance.\n",
        "\n",
        "    Attention Mechanism: This helps the model focus on the most emotionally relevant parts of the sequence‚Äîlike a spike in vocal pitch or a sudden facial reaction.\n",
        "\n",
        "    Fusion Layer: By combining all three modalities, your model builds a richer representation of the speaker‚Äôs emotional state.\n",
        "\n",
        "    Output Layer: Your final layer outputs logits for 7 classes, which correspond to the emotions you listed.\n",
        "\n",
        "üß† A Few Things to Double-Check\n",
        "\n",
        "    Label Encoding: Make sure your emotion labels are mapped consistently (e.g., {'Anger': 0, 'Disgust': 1, ..., 'Fear': 6}).\n",
        "\n",
        "    Loss Function: You're using CrossEntropyLoss, which is perfect for multi-class classification.\n",
        "\n",
        "    Evaluation: Use macro or weighted F1 score to account for class imbalance (some emotions like Disgust or Surprise might be rarer).\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEJJ1rA4c9Fm"
      },
      "outputs": [],
      "source": [
        "''' Step-by-Step: Preparing and Feeding Test Data\n",
        "1. Understand the Test Data Structure\n",
        "\n",
        "Your test CSV likely includes columns like:\n",
        "\n",
        "    Utterance: the spoken sentence (text modality)\n",
        "\n",
        "    Speaker: who said it\n",
        "\n",
        "    Emotion: ground truth label (for evaluation)\n",
        "\n",
        "    Dialogue_ID, Utterance_ID: for context tracking\n",
        "\n",
        "    Audio_Path or Video_Path: paths to audio/video files (if available)'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMt1NAJheF8t"
      },
      "outputs": [],
      "source": [
        "# from transformers import BertTokenizer\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# text_inputs = tokenizer(list(test_df['Utterance']), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# import torchaudio\n",
        "\n",
        "# def extract_audio_features(path):\n",
        "#     waveform, sample_rate = torchaudio.load(path)\n",
        "#     # Example: MFCCs\n",
        "#     mfcc = torchaudio.transforms.MFCC()(waveform)\n",
        "#     return mfcc.mean(dim=-1)  # shape: [channels, features]\n",
        "\n",
        "# audio_features = [extract_audio_features(f\"/content/MELD.Raw/audio/{uid}.wav\") for uid in test_df['Utterance_ID']]\n",
        "# Assuming precomputed video embeddings\n",
        "\n",
        "## If you're using the preprocessed features from test.pkl, then you do not need to manually extract audio features from MELD.Raw/audio/. The .pkl files already contain aligned and pre-extracted features for each modality: text, audio, and video.\n",
        "##video_features = torch.load(\"video_test_embeddings.pt\")  # shape: [num_samples, feature_dim]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEA8qUagfet9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('test.pkl', 'rb') as f:\n",
        "    test_features = pickle.load(f)\n",
        "\n",
        "test_df['key'] = test_df['Dialogue_ID'].astype(str) + '-' + test_df['Utterance_ID'].astype(str)\n",
        "\n",
        "text_feats = []\n",
        "audio_feats = []\n",
        "video_feats = []\n",
        "labels = []\n",
        "\n",
        "for key in test_df['key']:\n",
        "    sample = test_features.get(key)\n",
        "    if sample:\n",
        "        text_feats.append(sample['text'])     # shape: [768] or similar\n",
        "        audio_feats.append(sample['audio'])   # shape: [n_audio_features]\n",
        "        video_feats.append(sample['video'])   # shape: [n_video_features]\n",
        "        labels.append(sample['label'])        # optional, for evaluation\n",
        "import torch\n",
        "\n",
        "text_tensor = torch.tensor(text_feats)\n",
        "audio_tensor = torch.tensor(audio_feats)\n",
        "video_tensor = torch.tensor(video_feats)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(text_tensor, audio_tensor, video_tensor)\n",
        "    predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "emotion_map = {0: 'Anger', 1: 'Disgust', 2: 'Sadness', 3: 'Joy', 4: 'Neutral', 5: 'Surprise', 6: 'Fear'}\n",
        "predicted_emotions = [emotion_map[p.item()] for p in predictions]\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(labels, predicted_emotions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Bk-W2YPi8th"
      },
      "outputs": [],
      "source": [
        "### You‚Äôre not lying by saying you built a project with the help of tools. You‚Äôre being resourceful. And that‚Äôs exactly what good engineers do.\n",
        "''' You understand the architecture, the data flow, and the purpose of each component.\n",
        "\n",
        "You can explain your choices: why you used certain loss functions, how you handled class imbalance, etc.\n",
        "\n",
        "You‚Äôve experimented, tweaked, and learned from the process.'''\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
